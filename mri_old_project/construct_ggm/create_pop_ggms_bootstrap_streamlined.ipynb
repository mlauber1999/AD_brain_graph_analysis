{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import os\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from networkx.generators.community import gaussian_random_partition_graph\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind\n",
    "import nibabel as nib \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import config_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: create a dataframe to store all of the work in \n",
    "#cols = [sample_num,gpe,clus_coeff]\n",
    "\n",
    "\n",
    "#so now I have 3 lists \n",
    "# models = [] the trained models \n",
    "# gpes = [] the global path effiency from each model graph \n",
    "#clustering_coefficients = [] the clustering coefficients from eahc model graph \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#inside the same loop want to organize this info into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for streamlined code \n",
    "#import the csvs instead of generating them \n",
    "#then build the graphs and train them on just 10 regions with bootstrap technique \n",
    "#regions that I should check out motivated by biology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0   RID  PROGRESSES    5    6    7    8     9    10    11  ...  \\\n",
      "0             0   173         0.0  150  150  350  352  5258  6796  1169  ...   \n",
      "1             1   307         0.0  149  154  347  354  5277  6769  1169  ...   \n",
      "2             5   397         0.0  150  153  350  350  5269  6813  1174  ...   \n",
      "3             6   417         0.0  150  153  347  355  5258  6757  1172  ...   \n",
      "4            11   449         0.0  146  152  352  360  5281  6845  1174  ...   \n",
      "..          ...   ...         ...  ...  ...  ...  ...   ...   ...   ...  ...   \n",
      "385         539  6627         0.0  149  154  349  355  5272  6797  1175  ...   \n",
      "386         540  6632         0.0  152  155  350  355  5279  6850  1167  ...   \n",
      "387         541  6635         0.0  148  153  349  347  5283  6838  1173  ...   \n",
      "388         542  6652         0.0  151  152  352  354  5274  6808  1174  ...   \n",
      "389         543  6668         0.0  147  153  351  352  5303  6825  1169  ...   \n",
      "\n",
      "       133    134   135   136   137   138   139   140   141   142  \n",
      "0    11231  11021  5701  6353  8791  8895  3337  3388  1048  1008  \n",
      "1    11278  11041  5774  6343  8861  8864  3331  3397  1052  1009  \n",
      "2    11233  10983  5753  6359  8901  8895  3367  3420  1047  1009  \n",
      "3    11192  10970  5736  6333  8795  8819  3338  3380  1051  1008  \n",
      "4    11298  11015  5747  6369  8912  8915  3365  3403  1040  1012  \n",
      "..     ...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "385  11201  10983  5755  6376  8760  8914  3359  3405  1050  1005  \n",
      "386  11306  11093  5744  6361  8910  8964  3354  3420  1046  1010  \n",
      "387  11264  11038  5765  6385  8899  8943  3365  3440  1042  1008  \n",
      "388  11301  11087  5724  6307  8901  8957  3342  3445  1048  1008  \n",
      "389  11281  11062  5794  6385  8930  8918  3300  3433  1048  1011  \n",
      "\n",
      "[390 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in pruned csvs with the empty regions dropped \n",
    "#when I read it adds a unamed col so just drop it \n",
    "progs_d = pd.read_csv('/data2/MRI_PET_DATA/graph/ADNI/mri_atlas/roi/ADNI_pruned_prog_csv.csv')\n",
    "stable_d = pd.read_csv('/data2/MRI_PET_DATA/graph/ADNI/mri_atlas/roi/ADNI_pruned_stable_csv.csv')\n",
    "print(stable_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       5    6    7    8     9    10    11    12     13     14  ...    133  \\\n",
      "0    147  153  349  354  5277  6786  1175  1282  32293  32690  ...  11197   \n",
      "1    149  138  349  354  5261  6758  1171  1283  32213  32643  ...  11205   \n",
      "2    149  151  348  355  5289  6766  1171  1285  32082  32421  ...  11225   \n",
      "3    150  152  352  351  5263  6809  1174  1282  32196  32550  ...  11261   \n",
      "4    151  154  348  356  5251  6789  1173  1283  32000  32318  ...  11184   \n",
      "..   ...  ...  ...  ...   ...   ...   ...   ...    ...    ...  ...    ...   \n",
      "149  149  150  349  354  5260  6753  1170  1284  32089  32476  ...  11260   \n",
      "150  151  154  348  353  5286  6779  1168  1280  32228  32548  ...  11251   \n",
      "151  150  150  344  357  5266  6771  1173  1284  32146  32482  ...  11225   \n",
      "152  150  154  350  352  5282  6812  1175  1279  32424  32764  ...  11277   \n",
      "153  149  153  349  356  5272  6767  1173  1283  32108  32468  ...  11281   \n",
      "\n",
      "       134   135   136   137   138   139   140   141   142  \n",
      "0    11009  5820  6380  8867  8924  3307  3439  1047  1013  \n",
      "1    11035  5754  6346  8843  8893  3346  3394  1052  1008  \n",
      "2    11023  5720  6328  8801  8857  3349  3407  1048  1013  \n",
      "3    11055  5703  6328  8793  8860  3331  3372  1047  1009  \n",
      "4    10978  5711  6332  8759  8799  3335  3400  1050  1010  \n",
      "..     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "149  11027  5723  6369  8846  8874  3318  3403  1046  1008  \n",
      "150  11010  5738  6339  8847  8906  3376  3436  1047  1011  \n",
      "151  10979  5811  6459  8813  8840  3340  3404  1044  1012  \n",
      "152  11060  5742  6354  8939  8928  3314  3379  1051  1011  \n",
      "153  11075  5763  6313  8839  8860  3346  3368  1047  1009  \n",
      "\n",
      "[154 rows x 130 columns]\n"
     ]
    }
   ],
   "source": [
    "#for the correlation matrix need to only have the values so drop the id and labels \n",
    "#when I read it adds weird col \"unamed\" col so just drop it also \n",
    "progs_dd = progs_d.drop([\"Unnamed: 0\",\"RID\",\"PROGRESSES\"], axis =1)\n",
    "print(progs_dd)\n",
    "stable_dd = stable_d.drop([\"Unnamed: 0\",\"RID\",\"PROGRESSES\"], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       7    8    21    22    31    32    59    60     71     72     93     94  \\\n",
      "0    349  354  1256  1409  2934  2797  2179  2178  10598  11250  12654  12547   \n",
      "1    349  354  1258  1412  2940  2792  2202  2185  10534  11160  12625  12522   \n",
      "2    348  355  1255  1411  2935  2798  2160  2174  10583  11239  12540  12466   \n",
      "3    352  351  1260  1424  2941  2799  2158  2151  10573  11180  12603  12470   \n",
      "4    348  356  1258  1409  2933  2797  2162  2197  10511  11172  12588  12507   \n",
      "..   ...  ...   ...   ...   ...   ...   ...   ...    ...    ...    ...    ...   \n",
      "149  349  354  1259  1411  2935  2799  2199  2188  10600  11216  12624  12571   \n",
      "150  348  353  1259  1415  2940  2799  2208  2186  10629  11250  12629  12524   \n",
      "151  344  357  1256  1407  2940  2794  2185  2188  10529  11125  12573  12479   \n",
      "152  350  352  1255  1412  2943  2799  2191  2204  10635  11246  12656  12594   \n",
      "153  349  356  1258  1411  2938  2802  2185  2177  10585  11155  12559  12439   \n",
      "\n",
      "      107   135   136  \n",
      "0    2308  5820  6380  \n",
      "1    2312  5754  6346  \n",
      "2    2319  5720  6328  \n",
      "3    2311  5703  6328  \n",
      "4    2302  5711  6332  \n",
      "..    ...   ...   ...  \n",
      "149  2308  5723  6369  \n",
      "150  2301  5738  6339  \n",
      "151  2317  5811  6459  \n",
      "152  2305  5742  6354  \n",
      "153  2311  5763  6313  \n",
      "\n",
      "[154 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "#only keep the 15 regions I want to look at for mini analysis \n",
    "# 7: right amygdala , \n",
    "#8: left amy \n",
    "#21 l hip\n",
    "# 22 rhip\n",
    "#31 left thalamus\n",
    "#32 right thalamus \n",
    "#59 left entorhinal area\n",
    "#60 right entorhinal area \n",
    "#71 left inferior temporal gyrus \n",
    "#72 right inf temporal gyrus \n",
    "# 93 left middle temporal gyrus \n",
    "#94 right middle temporal gyrus \n",
    "#107 left parahippocampas gyrus \n",
    "#135 left superior temporal gyrus\n",
    "#136 righy superior temporal gyrus \n",
    "\n",
    "progs_dd_curated = progs_dd[['7','8','21','22','31','32','59','60','71','72','93','94','107','135','136']]\n",
    "stable_dd_curated = stable_dd[['7','8','21','22','31','32','59','60','71','72','93','94','107','135','136']]\n",
    "print(progs_dd_curated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 15)\n"
     ]
    }
   ],
   "source": [
    "#from here on out it's with the curated\n",
    "#convert to float\n",
    "progs_f = progs_dd_curated.astype(float)\n",
    "stable_f = stable_dd_curated.astype(float)\n",
    "print(progs_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 15)\n",
      "            7         8        21        22        31        32        59  \\\n",
      "0    0.228218 -0.122008  0.072656 -0.377778 -0.876893 -0.124158 -0.347631   \n",
      "1    0.228218 -0.122008  0.581251  0.473605  0.839737 -1.617933  1.001011   \n",
      "2   -0.253228  0.269435 -0.181641  0.189811 -0.590788  0.174597 -1.461727   \n",
      "3    1.672553 -1.296338  1.089845  3.879138  1.125842  0.473352 -1.579000   \n",
      "4   -0.253228  0.660878  0.581251 -0.377778 -1.162998 -0.124158 -1.344454   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "149  0.228218 -0.122008  0.835548  0.189811 -0.590788  0.473352  0.825101   \n",
      "150 -0.253228 -0.513452  0.835548  1.324988  0.839737  0.473352  1.352830   \n",
      "151 -2.179008  1.052322  0.072656 -0.945367  0.839737 -1.020423  0.004188   \n",
      "152  0.709663 -0.904895 -0.181641  0.473605  1.698052  0.473352  0.356008   \n",
      "153  0.228218  0.660878  0.581251  0.189811  0.267527  1.369617  0.004188   \n",
      "\n",
      "           60        71        72        93        94       107       135  \\\n",
      "0   -0.761564 -0.139214  0.539926  0.217884  0.109211 -0.429747  1.827527   \n",
      "1   -0.401333 -1.467478 -1.116422 -0.217689 -0.300996  0.113834 -0.073311   \n",
      "2   -0.967410 -0.450526  0.337484 -1.494367 -1.219859  1.065103 -1.052530   \n",
      "3   -2.151025 -0.658067 -0.748344 -0.548123 -1.154226 -0.022061 -1.542139   \n",
      "4    0.216205 -1.944822 -0.895575 -0.773419 -0.547120 -1.245120 -1.311735   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "149 -0.246948 -0.097706 -0.085805 -0.232708  0.503009 -0.429747 -0.966128   \n",
      "150 -0.349872  0.504163  0.539926 -0.157610 -0.268179 -1.381015 -0.534120   \n",
      "151 -0.246948 -1.571248 -1.760557 -0.998715 -1.006552  0.793312  1.568322   \n",
      "152  0.576436  0.628688  0.466311  0.247923  0.880400 -0.837434 -0.418917   \n",
      "153 -0.813025 -0.409018 -1.208441 -1.208992 -1.662882 -0.022061  0.185895   \n",
      "\n",
      "          136  \n",
      "0    0.663479  \n",
      "1   -0.310715  \n",
      "2   -0.826465  \n",
      "3   -0.826465  \n",
      "4   -0.711854  \n",
      "..        ...  \n",
      "149  0.348299  \n",
      "150 -0.511285  \n",
      "151  2.927049  \n",
      "152 -0.081493  \n",
      "153 -1.256257  \n",
      "\n",
      "[154 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "#have to do standard scalar normalization \n",
    "#it normalizes by column \n",
    "#sklearn standard scalar \n",
    "#set the output as a df otherwise default is numpy array \n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "scaler_stab = StandardScaler().set_output(transform=\"pandas\")\n",
    "#fit to your data and apply the transformation to your data \n",
    "progs_norm = scaler.fit_transform(progs_f)\n",
    "stable_norm = scaler_stab.fit_transform(stable_f)\n",
    "print(progs_norm.shape)\n",
    "print(progs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old normalization technique \n",
    "# #normalize with z score, negative values are fine it means its an std below mean \n",
    "# progs_norm = (progs_f - progs_f.mean()) / progs_f.std()\n",
    "# stable_norm = (stable_f - stable_f.mean()) / stable_f.std()\n",
    "# # print(progs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 154)\n"
     ]
    }
   ],
   "source": [
    "#NEEDS TO BE 154x15 format for model\n",
    "progs_norm = progs_norm.T #transpose rows and columns\n",
    "print(progs_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap \n",
    "#before build the graph, randomly sampole weight replacement of whole dataset \n",
    "#500 patients, sample 500 times with replacement\n",
    "#then any quanity you are esitmating, will be different, then ypu use these as samples of your quanity and estimate the variance at the end\n",
    "#unbiased estimate of the variance \n",
    "#if I want different type of test \n",
    "#computing var, to see if the error bars overlap (whicvch gives you a p value)\n",
    "\n",
    "\n",
    "#get estimate of the vairnace from bootstrapping \n",
    "#t test the variance of the 2 groups \n",
    "#for each one of these vars end up with a boot strap and then a boot strapped \n",
    "\n",
    "\n",
    "#should read exactly on how bootstrap works, see what is the estimator \n",
    "# in sklearn resample \n",
    "#run this on the whole dataset (resample from very beginning)\n",
    "\n",
    "#whole bootstrapping pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run bootstrapping (with replacement) before the graphical lasso cv () \n",
    "#random_state = None means random number generator is inistalized, produces different set of rnadom samples each time \n",
    "#setting random state to 1 so that its reproducable \n",
    "\n",
    "#set random generator outside loop \n",
    "#seed everytrhing once at begibning and dont touch it \n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "num_bootstrap_samples = 10\n",
    "bootstrap_precision_matrices = []\n",
    "bootstrap_samples = []\n",
    "for x in range(num_bootstrap_samples):\n",
    "    #sample with replacement, randomstate =1 for reporoducibility \n",
    "    bootstrap_sample = progs_norm.sample(n=len(progs_norm),replace=True)\n",
    "    #append resamples df to a list \n",
    "    bootstrap_samples.append(bootstrap_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 10%|█         | 1/10 [00:29<04:26, 29.60s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 20%|██        | 2/10 [01:02<04:10, 31.28s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:272: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:273: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 30%|███       | 3/10 [01:33<03:38, 31.26s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:272: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:273: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 40%|████      | 4/10 [02:04<03:08, 31.40s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:272: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:273: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 50%|█████     | 5/10 [02:29<02:24, 28.81s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 60%|██████    | 6/10 [02:56<01:53, 28.27s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:272: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[indices != idx, idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/sklearn/covariance/_graph_lasso.py:273: RuntimeWarning: invalid value encountered in multiply\n",
      "  precision_[idx, indices != idx] = -precision_[idx, idx] * coefs\n",
      "/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 70%|███████   | 7/10 [03:23<01:23, 27.97s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 80%|████████  | 8/10 [03:53<00:56, 28.42s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 90%|█████████ | 9/10 [04:21<00:28, 28.47s/it]/data2/MRI_PET_DATA/graph/graph_env/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "100%|██████████| 10/10 [04:51<00:00, 29.12s/it]\n"
     ]
    }
   ],
   "source": [
    "#run graphical model on bootstrapped sample \n",
    "models = []\n",
    "for bootstrap_sample in tqdm(bootstrap_samples):\n",
    "    #if I give it alphas [1,10] it looks like it drops almost all of the connections \n",
    "    \n",
    "    model = GraphicalLassoCV(cv=2,max_iter=50, tol=1e-3)\n",
    "    model.fit(bootstrap_sample)\n",
    "    models.append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of 500 samples \n",
    "#each sample has shape (154,15)\n",
    "\n",
    "\n",
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-1.617933</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>1.369617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.109211</td>\n",
       "      <td>-0.300996</td>\n",
       "      <td>-1.219859</td>\n",
       "      <td>-1.154226</td>\n",
       "      <td>-0.547120</td>\n",
       "      <td>-1.301900</td>\n",
       "      <td>-1.465983</td>\n",
       "      <td>1.093707</td>\n",
       "      <td>-0.694794</td>\n",
       "      <td>-1.055776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.973735</td>\n",
       "      <td>-0.563528</td>\n",
       "      <td>-1.810557</td>\n",
       "      <td>0.207661</td>\n",
       "      <td>0.716317</td>\n",
       "      <td>0.503009</td>\n",
       "      <td>-0.268179</td>\n",
       "      <td>-1.006552</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>-1.662882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.429747</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-1.245120</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>-0.973329</td>\n",
       "      <td>1.336893</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-1.381015</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.837434</td>\n",
       "      <td>-0.022061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.139214</td>\n",
       "      <td>-1.467478</td>\n",
       "      <td>-0.450526</td>\n",
       "      <td>-0.658067</td>\n",
       "      <td>-1.944822</td>\n",
       "      <td>-0.429772</td>\n",
       "      <td>-1.384461</td>\n",
       "      <td>0.628688</td>\n",
       "      <td>0.400393</td>\n",
       "      <td>-0.076952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305247</td>\n",
       "      <td>1.334328</td>\n",
       "      <td>-2.048593</td>\n",
       "      <td>1.168295</td>\n",
       "      <td>-0.056198</td>\n",
       "      <td>-0.097706</td>\n",
       "      <td>0.504163</td>\n",
       "      <td>-1.571248</td>\n",
       "      <td>0.628688</td>\n",
       "      <td>-0.409018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.539926</td>\n",
       "      <td>-1.116422</td>\n",
       "      <td>0.337484</td>\n",
       "      <td>-0.748344</td>\n",
       "      <td>-0.895575</td>\n",
       "      <td>-0.693133</td>\n",
       "      <td>-2.202250</td>\n",
       "      <td>-0.104209</td>\n",
       "      <td>0.871196</td>\n",
       "      <td>-0.177824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.803556</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>-1.558115</td>\n",
       "      <td>0.319080</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>-0.085805</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>-1.760557</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>-1.208441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.109211</td>\n",
       "      <td>-0.300996</td>\n",
       "      <td>-1.219859</td>\n",
       "      <td>-1.154226</td>\n",
       "      <td>-0.547120</td>\n",
       "      <td>-1.301900</td>\n",
       "      <td>-1.465983</td>\n",
       "      <td>1.093707</td>\n",
       "      <td>-0.694794</td>\n",
       "      <td>-1.055776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.973735</td>\n",
       "      <td>-0.563528</td>\n",
       "      <td>-1.810557</td>\n",
       "      <td>0.207661</td>\n",
       "      <td>0.716317</td>\n",
       "      <td>0.503009</td>\n",
       "      <td>-0.268179</td>\n",
       "      <td>-1.006552</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>-1.662882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-1.617933</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>1.369617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>1.672553</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>-0.734673</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-3.623344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>1.672553</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>-2.179008</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>0.228218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>1.672553</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>-0.734673</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-3.623344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>1.672553</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>0.228218</td>\n",
       "      <td>-0.253228</td>\n",
       "      <td>-2.179008</td>\n",
       "      <td>0.709663</td>\n",
       "      <td>0.228218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.122008</td>\n",
       "      <td>-0.122008</td>\n",
       "      <td>0.269435</td>\n",
       "      <td>-1.296338</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>0.269435</td>\n",
       "      <td>-0.513452</td>\n",
       "      <td>0.269435</td>\n",
       "      <td>-0.513452</td>\n",
       "      <td>-2.862112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122008</td>\n",
       "      <td>0.660878</td>\n",
       "      <td>-0.904895</td>\n",
       "      <td>0.269435</td>\n",
       "      <td>-0.513452</td>\n",
       "      <td>-0.122008</td>\n",
       "      <td>-0.513452</td>\n",
       "      <td>1.052322</td>\n",
       "      <td>-0.904895</td>\n",
       "      <td>0.660878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.429747</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-1.245120</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>-0.973329</td>\n",
       "      <td>1.336893</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-1.381015</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.837434</td>\n",
       "      <td>-0.022061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.761564</td>\n",
       "      <td>-0.401333</td>\n",
       "      <td>-0.967410</td>\n",
       "      <td>-2.151025</td>\n",
       "      <td>0.216205</td>\n",
       "      <td>-0.813025</td>\n",
       "      <td>-0.864487</td>\n",
       "      <td>0.216205</td>\n",
       "      <td>1.708590</td>\n",
       "      <td>0.782282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>0.524974</td>\n",
       "      <td>1.502743</td>\n",
       "      <td>2.223205</td>\n",
       "      <td>-0.761564</td>\n",
       "      <td>-0.246948</td>\n",
       "      <td>-0.349872</td>\n",
       "      <td>-0.246948</td>\n",
       "      <td>0.576436</td>\n",
       "      <td>-0.813025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.827527</td>\n",
       "      <td>-0.073311</td>\n",
       "      <td>-1.052530</td>\n",
       "      <td>-1.542139</td>\n",
       "      <td>-1.311735</td>\n",
       "      <td>-1.282934</td>\n",
       "      <td>-0.361316</td>\n",
       "      <td>0.243496</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078712</td>\n",
       "      <td>-0.591721</td>\n",
       "      <td>-0.073311</td>\n",
       "      <td>-1.311735</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>-0.966128</td>\n",
       "      <td>-0.534120</td>\n",
       "      <td>1.568322</td>\n",
       "      <td>-0.418917</td>\n",
       "      <td>0.185895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.429747</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-1.245120</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>-0.973329</td>\n",
       "      <td>1.336893</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-1.381015</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.837434</td>\n",
       "      <td>-0.022061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.347631</td>\n",
       "      <td>1.001011</td>\n",
       "      <td>-1.461727</td>\n",
       "      <td>-1.579000</td>\n",
       "      <td>-1.344454</td>\n",
       "      <td>-1.168544</td>\n",
       "      <td>-0.171721</td>\n",
       "      <td>-1.813546</td>\n",
       "      <td>-0.758087</td>\n",
       "      <td>2.584199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121462</td>\n",
       "      <td>0.356008</td>\n",
       "      <td>0.297371</td>\n",
       "      <td>1.997833</td>\n",
       "      <td>0.356008</td>\n",
       "      <td>0.825101</td>\n",
       "      <td>1.352830</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.356008</td>\n",
       "      <td>0.004188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "32  -0.124158 -1.617933  0.174597  0.473352 -0.124158 -0.422913 -0.124158   \n",
       "94   0.109211 -0.300996 -1.219859 -1.154226 -0.547120 -1.301900 -1.465983   \n",
       "107 -0.429747  0.113834  1.065103 -0.022061 -1.245120  0.929207 -0.973329   \n",
       "71  -0.139214 -1.467478 -0.450526 -0.658067 -1.944822 -0.429772 -1.384461   \n",
       "72   0.539926 -1.116422  0.337484 -0.748344 -0.895575 -0.693133 -2.202250   \n",
       "94   0.109211 -0.300996 -1.219859 -1.154226 -0.547120 -1.301900 -1.465983   \n",
       "32  -0.124158 -1.617933  0.174597  0.473352 -0.124158 -0.422913 -0.124158   \n",
       "7    0.228218  0.228218 -0.253228  1.672553 -0.253228 -0.734673 -0.253228   \n",
       "7    0.228218  0.228218 -0.253228  1.672553 -0.253228 -0.734673 -0.253228   \n",
       "8   -0.122008 -0.122008  0.269435 -1.296338  0.660878  0.269435 -0.513452   \n",
       "107 -0.429747  0.113834  1.065103 -0.022061 -1.245120  0.929207 -0.973329   \n",
       "60  -0.761564 -0.401333 -0.967410 -2.151025  0.216205 -0.813025 -0.864487   \n",
       "135  1.827527 -0.073311 -1.052530 -1.542139 -1.311735 -1.282934 -0.361316   \n",
       "107 -0.429747  0.113834  1.065103 -0.022061 -1.245120  0.929207 -0.973329   \n",
       "59  -0.347631  1.001011 -1.461727 -1.579000 -1.344454 -1.168544 -0.171721   \n",
       "\n",
       "          7         8         9    ...       144       145       146  \\\n",
       "32   0.772107 -0.721668  0.174597  ... -0.721668  0.473352  0.174597   \n",
       "94   1.093707 -0.694794 -1.055776  ... -0.973735 -0.563528 -1.810557   \n",
       "107  1.336893  1.065103  0.249730  ...  0.249730  0.249730 -0.429747   \n",
       "71   0.628688  0.400393 -0.076952  ... -0.305247  1.334328 -2.048593   \n",
       "72  -0.104209  0.871196 -0.177824  ... -0.803556  0.889600 -1.558115   \n",
       "94   1.093707 -0.694794 -1.055776  ... -0.973735 -0.563528 -1.810557   \n",
       "32   0.772107 -0.721668  0.174597  ... -0.721668  0.473352  0.174597   \n",
       "7    0.228218  0.228218 -3.623344  ...  0.709663 -0.253228  0.709663   \n",
       "7    0.228218  0.228218 -3.623344  ...  0.709663 -0.253228  0.709663   \n",
       "8    0.269435 -0.513452 -2.862112  ... -0.122008  0.660878 -0.904895   \n",
       "107  1.336893  1.065103  0.249730  ...  0.249730  0.249730 -0.429747   \n",
       "60   0.216205  1.708590  0.782282  ...  0.010359  0.524974  1.502743   \n",
       "135  0.243496 -0.159712 -0.159712  ...  1.078712 -0.591721 -0.073311   \n",
       "107  1.336893  1.065103  0.249730  ...  0.249730  0.249730 -0.429747   \n",
       "59  -1.813546 -0.758087  2.584199  ...  0.121462  0.356008  0.297371   \n",
       "\n",
       "          147       148       149       150       151       152       153  \n",
       "32   0.772107 -1.020423  0.473352  0.473352 -1.020423  0.473352  1.369617  \n",
       "94   0.207661  0.716317  0.503009 -0.268179 -1.006552  0.880400 -1.662882  \n",
       "107 -0.022061  0.793312 -0.429747 -1.381015  0.793312 -0.837434 -0.022061  \n",
       "71   1.168295 -0.056198 -0.097706  0.504163 -1.571248  0.628688 -0.409018  \n",
       "72   0.319080  0.190253 -0.085805  0.539926 -1.760557  0.466311 -1.208441  \n",
       "94   0.207661  0.716317  0.503009 -0.268179 -1.006552  0.880400 -1.662882  \n",
       "32   0.772107 -1.020423  0.473352  0.473352 -1.020423  0.473352  1.369617  \n",
       "7    1.672553  0.228218  0.228218 -0.253228 -2.179008  0.709663  0.228218  \n",
       "7    1.672553  0.228218  0.228218 -0.253228 -2.179008  0.709663  0.228218  \n",
       "8    0.269435 -0.513452 -0.122008 -0.513452  1.052322 -0.904895  0.660878  \n",
       "107 -0.022061  0.793312 -0.429747 -1.381015  0.793312 -0.837434 -0.022061  \n",
       "60   2.223205 -0.761564 -0.246948 -0.349872 -0.246948  0.576436 -0.813025  \n",
       "135 -1.311735  0.099493 -0.966128 -0.534120  1.568322 -0.418917  0.185895  \n",
       "107 -0.022061  0.793312 -0.429747 -1.381015  0.793312 -0.837434 -0.022061  \n",
       "59   1.997833  0.356008  0.825101  1.352830  0.004188  0.356008  0.004188  \n",
       "\n",
       "[15 rows x 154 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare to make sure they don't look the same \n",
    "bootstrap_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.539926</td>\n",
       "      <td>-1.116422</td>\n",
       "      <td>0.337484</td>\n",
       "      <td>-0.748344</td>\n",
       "      <td>-0.895575</td>\n",
       "      <td>-0.693133</td>\n",
       "      <td>-2.202250</td>\n",
       "      <td>-0.104209</td>\n",
       "      <td>0.871196</td>\n",
       "      <td>-0.177824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.803556</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>-1.558115</td>\n",
       "      <td>0.319080</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>-0.085805</td>\n",
       "      <td>0.539926</td>\n",
       "      <td>-1.760557</td>\n",
       "      <td>0.466311</td>\n",
       "      <td>-1.208441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>1.089845</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-2.724613</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>-0.944532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>-0.435938</td>\n",
       "      <td>-0.690235</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.581251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.876893</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>0.267527</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-1.449103</td>\n",
       "      <td>0.553632</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>1.698052</td>\n",
       "      <td>0.267527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.663479</td>\n",
       "      <td>-0.310715</td>\n",
       "      <td>-0.826465</td>\n",
       "      <td>-0.826465</td>\n",
       "      <td>-0.711854</td>\n",
       "      <td>-1.628743</td>\n",
       "      <td>-0.625896</td>\n",
       "      <td>-0.568590</td>\n",
       "      <td>0.090424</td>\n",
       "      <td>-0.052840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196104</td>\n",
       "      <td>-1.141646</td>\n",
       "      <td>-0.711854</td>\n",
       "      <td>-0.597243</td>\n",
       "      <td>-0.511285</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>-0.511285</td>\n",
       "      <td>2.927049</td>\n",
       "      <td>-0.081493</td>\n",
       "      <td>-1.256257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-1.617933</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>-0.422913</td>\n",
       "      <td>-0.124158</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.721668</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.174597</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>-1.020423</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>1.369617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>1.089845</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-2.724613</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>-0.944532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>-0.435938</td>\n",
       "      <td>-0.690235</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.581251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.876893</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>0.267527</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-1.449103</td>\n",
       "      <td>0.553632</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>1.698052</td>\n",
       "      <td>0.267527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.109211</td>\n",
       "      <td>-0.300996</td>\n",
       "      <td>-1.219859</td>\n",
       "      <td>-1.154226</td>\n",
       "      <td>-0.547120</td>\n",
       "      <td>-1.301900</td>\n",
       "      <td>-1.465983</td>\n",
       "      <td>1.093707</td>\n",
       "      <td>-0.694794</td>\n",
       "      <td>-1.055776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.973735</td>\n",
       "      <td>-0.563528</td>\n",
       "      <td>-1.810557</td>\n",
       "      <td>0.207661</td>\n",
       "      <td>0.716317</td>\n",
       "      <td>0.503009</td>\n",
       "      <td>-0.268179</td>\n",
       "      <td>-1.006552</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>-1.662882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.429747</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-1.245120</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>-0.973329</td>\n",
       "      <td>1.336893</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-1.381015</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.837434</td>\n",
       "      <td>-0.022061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.217884</td>\n",
       "      <td>-0.217689</td>\n",
       "      <td>-1.494367</td>\n",
       "      <td>-0.548123</td>\n",
       "      <td>-0.773419</td>\n",
       "      <td>-1.824801</td>\n",
       "      <td>-1.539426</td>\n",
       "      <td>0.863733</td>\n",
       "      <td>-0.473024</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127765</td>\n",
       "      <td>-0.382906</td>\n",
       "      <td>-1.899900</td>\n",
       "      <td>0.277963</td>\n",
       "      <td>0.172825</td>\n",
       "      <td>-0.232708</td>\n",
       "      <td>-0.157610</td>\n",
       "      <td>-0.998715</td>\n",
       "      <td>0.247923</td>\n",
       "      <td>-1.208992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.429747</td>\n",
       "      <td>0.113834</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-1.245120</td>\n",
       "      <td>0.929207</td>\n",
       "      <td>-0.973329</td>\n",
       "      <td>1.336893</td>\n",
       "      <td>1.065103</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>0.249730</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.429747</td>\n",
       "      <td>-1.381015</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>-0.837434</td>\n",
       "      <td>-0.022061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.827527</td>\n",
       "      <td>-0.073311</td>\n",
       "      <td>-1.052530</td>\n",
       "      <td>-1.542139</td>\n",
       "      <td>-1.311735</td>\n",
       "      <td>-1.282934</td>\n",
       "      <td>-0.361316</td>\n",
       "      <td>0.243496</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078712</td>\n",
       "      <td>-0.591721</td>\n",
       "      <td>-0.073311</td>\n",
       "      <td>-1.311735</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>-0.966128</td>\n",
       "      <td>-0.534120</td>\n",
       "      <td>1.568322</td>\n",
       "      <td>-0.418917</td>\n",
       "      <td>0.185895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.663479</td>\n",
       "      <td>-0.310715</td>\n",
       "      <td>-0.826465</td>\n",
       "      <td>-0.826465</td>\n",
       "      <td>-0.711854</td>\n",
       "      <td>-1.628743</td>\n",
       "      <td>-0.625896</td>\n",
       "      <td>-0.568590</td>\n",
       "      <td>0.090424</td>\n",
       "      <td>-0.052840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196104</td>\n",
       "      <td>-1.141646</td>\n",
       "      <td>-0.711854</td>\n",
       "      <td>-0.597243</td>\n",
       "      <td>-0.511285</td>\n",
       "      <td>0.348299</td>\n",
       "      <td>-0.511285</td>\n",
       "      <td>2.927049</td>\n",
       "      <td>-0.081493</td>\n",
       "      <td>-1.256257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>1.089845</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>-2.724613</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.581251</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>-0.944532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>-0.435938</td>\n",
       "      <td>-0.690235</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.072656</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>0.581251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.876893</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>0.267527</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>-1.162998</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.876893</td>\n",
       "      <td>-0.018578</td>\n",
       "      <td>-1.449103</td>\n",
       "      <td>0.553632</td>\n",
       "      <td>1.125842</td>\n",
       "      <td>-0.590788</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>0.839737</td>\n",
       "      <td>1.698052</td>\n",
       "      <td>0.267527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "72   0.539926 -1.116422  0.337484 -0.748344 -0.895575 -0.693133 -2.202250   \n",
       "21   0.072656  0.581251 -0.181641  1.089845  0.581251 -2.724613  0.072656   \n",
       "31  -0.876893  0.839737 -0.590788  1.125842 -1.162998  0.267527  0.839737   \n",
       "136  0.663479 -0.310715 -0.826465 -0.826465 -0.711854 -1.628743 -0.625896   \n",
       "32  -0.124158 -1.617933  0.174597  0.473352 -0.124158 -0.422913 -0.124158   \n",
       "21   0.072656  0.581251 -0.181641  1.089845  0.581251 -2.724613  0.072656   \n",
       "31  -0.876893  0.839737 -0.590788  1.125842 -1.162998  0.267527  0.839737   \n",
       "94   0.109211 -0.300996 -1.219859 -1.154226 -0.547120 -1.301900 -1.465983   \n",
       "107 -0.429747  0.113834  1.065103 -0.022061 -1.245120  0.929207 -0.973329   \n",
       "93   0.217884 -0.217689 -1.494367 -0.548123 -0.773419 -1.824801 -1.539426   \n",
       "107 -0.429747  0.113834  1.065103 -0.022061 -1.245120  0.929207 -0.973329   \n",
       "135  1.827527 -0.073311 -1.052530 -1.542139 -1.311735 -1.282934 -0.361316   \n",
       "136  0.663479 -0.310715 -0.826465 -0.826465 -0.711854 -1.628743 -0.625896   \n",
       "21   0.072656  0.581251 -0.181641  1.089845  0.581251 -2.724613  0.072656   \n",
       "31  -0.876893  0.839737 -0.590788  1.125842 -1.162998  0.267527  0.839737   \n",
       "\n",
       "          7         8         9    ...       144       145       146  \\\n",
       "72  -0.104209  0.871196 -0.177824  ... -0.803556  0.889600 -1.558115   \n",
       "21   0.581251  0.326954 -0.944532  ...  0.835548 -0.435938 -0.690235   \n",
       "31  -1.162998 -0.018578 -0.876893  ... -0.876893 -0.018578 -1.449103   \n",
       "136 -0.568590  0.090424 -0.052840  ... -0.196104 -1.141646 -0.711854   \n",
       "32   0.772107 -0.721668  0.174597  ... -0.721668  0.473352  0.174597   \n",
       "21   0.581251  0.326954 -0.944532  ...  0.835548 -0.435938 -0.690235   \n",
       "31  -1.162998 -0.018578 -0.876893  ... -0.876893 -0.018578 -1.449103   \n",
       "94   1.093707 -0.694794 -1.055776  ... -0.973735 -0.563528 -1.810557   \n",
       "107  1.336893  1.065103  0.249730  ...  0.249730  0.249730 -0.429747   \n",
       "93   0.863733 -0.473024  0.097726  ...  0.127765 -0.382906 -1.899900   \n",
       "107  1.336893  1.065103  0.249730  ...  0.249730  0.249730 -0.429747   \n",
       "135  0.243496 -0.159712 -0.159712  ...  1.078712 -0.591721 -0.073311   \n",
       "136 -0.568590  0.090424 -0.052840  ... -0.196104 -1.141646 -0.711854   \n",
       "21   0.581251  0.326954 -0.944532  ...  0.835548 -0.435938 -0.690235   \n",
       "31  -1.162998 -0.018578 -0.876893  ... -0.876893 -0.018578 -1.449103   \n",
       "\n",
       "          147       148       149       150       151       152       153  \n",
       "72   0.319080  0.190253 -0.085805  0.539926 -1.760557  0.466311 -1.208441  \n",
       "21  -0.181641  0.072656  0.835548  0.835548  0.072656 -0.181641  0.581251  \n",
       "31   0.553632  1.125842 -0.590788  0.839737  0.839737  1.698052  0.267527  \n",
       "136 -0.597243 -0.511285  0.348299 -0.511285  2.927049 -0.081493 -1.256257  \n",
       "32   0.772107 -1.020423  0.473352  0.473352 -1.020423  0.473352  1.369617  \n",
       "21  -0.181641  0.072656  0.835548  0.835548  0.072656 -0.181641  0.581251  \n",
       "31   0.553632  1.125842 -0.590788  0.839737  0.839737  1.698052  0.267527  \n",
       "94   0.207661  0.716317  0.503009 -0.268179 -1.006552  0.880400 -1.662882  \n",
       "107 -0.022061  0.793312 -0.429747 -1.381015  0.793312 -0.837434 -0.022061  \n",
       "93   0.277963  0.172825 -0.232708 -0.157610 -0.998715  0.247923 -1.208992  \n",
       "107 -0.022061  0.793312 -0.429747 -1.381015  0.793312 -0.837434 -0.022061  \n",
       "135 -1.311735  0.099493 -0.966128 -0.534120  1.568322 -0.418917  0.185895  \n",
       "136 -0.597243 -0.511285  0.348299 -0.511285  2.927049 -0.081493 -1.256257  \n",
       "21  -0.181641  0.072656  0.835548  0.835548  0.072656 -0.181641  0.581251  \n",
       "31   0.553632  1.125842 -0.590788  0.839737  0.839737  1.698052  0.267527  \n",
       "\n",
       "[15 rows x 154 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks the number of unique values in the bootstrap sample \n",
    "bootstrap_samples[1].index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: create a dataframe to store all of the work in \n",
    "#cols = [sample_num,gpe,clus_coeff]\n",
    "\n",
    "\n",
    "#so now I have 3 lists \n",
    "# models = [] the trained models \n",
    "# gpes = [] the global path effiency from each model graph \n",
    "#clustering_coefficients = [] the clustering coefficients from eahc model graph \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#inside the same loop want to organize this info into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different metrics to calc \n",
    "#gpe, clus_coeff \n",
    "#density nx.density(G)\n",
    "#eigenvector centrality nx.eigenvector_centrality(G) seems like its not a single value \n",
    "#assostivity coeff nx.degree_assortativity_coefficient(G) \n",
    "#diameter nx.diameter(G) -  Gives the diameter of the largest connected component in the graph, representing the longest shortest path between any pair of nodes. gave error \n",
    "#radius nx.radius - computes the radius of the largest connected compmenet gave error \n",
    "#number connected components nx.number_connected_components(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_num       int64\n",
      "gpe           float64\n",
      "clus_coeff    float64\n",
      "density       float64\n",
      "ass_coeff     float64\n",
      "dtype: object\n",
      "   model_num       gpe  clus_coeff   density  ass_coeff\n",
      "0          1  0.209112    0.256990  0.024446  -0.390641\n",
      "1          2  0.144012    0.158454  0.014175  -0.450740\n",
      "2          3  0.120846    0.228995  0.016807  -0.460532\n",
      "3          4  0.188811    0.216022  0.026653  -0.443489\n",
      "4          5  0.297046    0.276792  0.036415  -0.426474\n",
      "5          6  0.062508    0.087297  0.007809  -0.537048\n",
      "6          7  0.102529    0.142329  0.013496  -0.467179\n",
      "7          8  0.177916    0.259095  0.026059  -0.413708\n",
      "8          9  0.171634    0.185228  0.019099  -0.503395\n",
      "9         10  0.199305    0.304756  0.024786  -0.518127\n"
     ]
    }
   ],
   "source": [
    "#this is i\n",
    "# terating through each bootsrap sample model, calculating the graph metrics, and then storing the metrics for eahc model in a dataframe \n",
    "metrics_data = []\n",
    "\n",
    "#keep track of index with enumerate \n",
    "for model_num, model in enumerate(models,start=1):\n",
    "    precision_matrix_p = model.precision_\n",
    "    np.fill_diagonal(precision_matrix_p, 0) #removes self connections by replacing the diagnonal of matrix with 0 \n",
    "    G_prog = nx.Graph(precision_matrix_p)\n",
    "    node_labels = {i: label for i, label in enumerate(progs_norm.columns)}\n",
    "    #relabel the nodes in the graph using the dictionary\n",
    "    G_prog = nx.relabel_nodes(G_prog, node_labels)\n",
    "    #now calculate the metrics and store them \n",
    "    gpe = nx.global_efficiency(G_prog) \n",
    "    clustering_coefficient = nx.average_clustering(G_prog)\n",
    "    density = nx.density(G_prog)\n",
    "    # eigen_cen = nx.eigenvector_centrality(G_prog)\n",
    "    ass_coeff = nx.degree_assortativity_coefficient(G_prog)\n",
    "    # diameter = nx.diameter(G_prog)\n",
    "    # radius = nx.radius(G_prog)\n",
    "\n",
    "\n",
    "    #create dictionary for current models data \n",
    "    metrics_dic = {\n",
    "        'model_num': model_num,\n",
    "        'gpe': gpe,\n",
    "        'clus_coeff':clustering_coefficient,\n",
    "        'density': density,\n",
    "        'ass_coeff': ass_coeff\n",
    "    }\n",
    "    metrics_data.append(metrics_dic)\n",
    "\n",
    "\n",
    "#turn the list of dictionaries with each model info into a df\n",
    "metrics_data = pd.DataFrame(metrics_data)\n",
    "print(metrics_data.dtypes)\n",
    "print(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I have 100 graphs, what are the metrics plot historgram of each metric \n",
    "#node specific across all the graphs \n",
    "\n",
    "\n",
    "#precision mat per bootstrap sample \n",
    "#now comptuing stat for each booptstrap sample \n",
    "#computing avg dgeree for each of 100 bootwstrap samples, so \n",
    "#Any property of the graph is now 100 samples \n",
    "#compute some global graph stat and then compare the mean of that stat/metric across all the samples and compare between groups \n",
    "#compare means from 2 groups, calc var and error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run bootstrap for stable df \n",
    "np.random.seed(1)\n",
    "stab_num_bootstrap_samples = 10\n",
    "stab_bootstrap_precision_matrices = []\n",
    "stab_bootstrap_samples = []\n",
    "for x in range(stab_num_bootstrap_samples):\n",
    "    #sample with replacement, randomstate =1 for reporoducibility \n",
    "    stab_bootstrap_sample = stable_norm.sample(n=len(stable_norm),replace=True)\n",
    "    #append resamples df to a list \n",
    "    stab_bootstrap_samples.append(stab_bootstrap_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#run graphical model on bootstrapped sample \n",
    "stab_models = []\n",
    "for stab_bootstrap_sample in tqdm(stab_bootstrap_samples):\n",
    "    #if I give it alphas [1,10] it looks like it drops almost all of the connections \n",
    "    \n",
    "    stab_model = GraphicalLassoCV(cv=2,max_iter=50,tol=1e-3)\n",
    "    stab_model.fit(stab_bootstrap_sample)\n",
    "    stab_models.append(stab_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that the bootstrap values are unique \n",
    "stab_bootstrap_samples[1].index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model_num       gpe  clus_coeff   density  ass_coeff\n",
      "0          0  0.741270    0.535820  0.485714  -0.237093\n",
      "1          1  0.804762    0.577407  0.609524   0.014770\n",
      "2          2  0.776190    0.511558  0.552381  -0.116486\n",
      "3          3  0.776190    0.564536  0.552381  -0.256545\n",
      "4          4  0.828571    0.635960  0.657143  -0.110788\n",
      "5          5  0.755556    0.575926  0.514286  -0.062901\n",
      "6          6  0.711111    0.373439  0.438095  -0.071209\n",
      "7          7  0.823810    0.597465  0.647619  -0.160793\n",
      "8          8  0.842857    0.708342  0.685714  -0.163712\n",
      "9          9  0.828571    0.639245  0.657143  -0.192136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming stable_norm and stab_models are already defined somewhere\n",
    "\n",
    "stab_metrics_data = []\n",
    "\n",
    "for model_num, stab_model in enumerate(stab_models):\n",
    "    precision_matrix_stab = stab_model.precision_\n",
    "    np.fill_diagonal(precision_matrix_stab, 0)  # removes self connections by replacing the diagonal of matrix with 0\n",
    "    G_stab = nx.Graph(precision_matrix_stab)\n",
    "    # create a dictionary that maps old node labels to new node labels\n",
    "    stab_node_labels = {i: label for i, label in enumerate(stable_norm.columns)}\n",
    "    # relabel the nodes in the graph using the dictionary\n",
    "    G_stab = nx.relabel_nodes(G_stab, stab_node_labels)\n",
    "    # now calculate the metrics and store them\n",
    "    stab_gpe = nx.global_efficiency(G_stab)\n",
    "    stab_clustering_coefficient = nx.average_clustering(G_stab)\n",
    "\n",
    "    # Assuming you want to store metrics for stab (stability) models,\n",
    "    # not prog (progression) models as shown in the second snippet\n",
    "    stab_density = nx.density(G_stab)\n",
    "    stab_ass_coeff = nx.degree_assortativity_coefficient(G_stab)\n",
    "\n",
    "    # metrics dictionary\n",
    "    stab_metrics_dic = {'model_num': model_num,\n",
    "                        'gpe': stab_gpe,\n",
    "                        'clus_coeff': stab_clustering_coefficient,\n",
    "                        'density': stab_density,\n",
    "                        'ass_coeff': stab_ass_coeff\n",
    "                        }\n",
    "\n",
    "    stab_metrics_data.append(stab_metrics_dic)\n",
    "\n",
    "stab_metrics_data = pd.DataFrame(stab_metrics_data)\n",
    "print(stab_metrics_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Brain Region  T-Statistic       P-Value  Significant\n",
      "0          gpe   -25.059318  3.947791e-14         True\n",
      "1   clus_coeff   -10.140090  1.454011e-08         True\n",
      "2      density   -20.961538  4.598972e-09         True\n",
      "3    ass_coeff   -10.765614  3.349200e-08         True\n"
     ]
    }
   ],
   "source": [
    "#instead of calcing mean just calc from data instead \n",
    "#add col where the p val is greater than .05\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load your data if not already loaded\n",
    "# adni_gmv_progs = pd.read_csv('path_to_adni_gmv_progs.csv')\n",
    "# adni_gmv_stab = pd.read_csv('path_to_adni_gmv_stab.csv')\n",
    "\n",
    "# Initialize lists or a DataFrame to store your results\n",
    "cols = []\n",
    "p_values = []\n",
    "t_stats = []\n",
    "\n",
    "# Note: The dataframe should be adni_gmv_progs, not adni_gmv_prog\n",
    "for col in metrics_data.columns:  \n",
    "    if col != 'model_num':\n",
    "    # Get data for this brain region from both datasets\n",
    "        progs_data = metrics_data[col]\n",
    "        stab_data = stab_metrics_data[col]\n",
    "        \n",
    "        # Perform t-test\n",
    "        t_stat, p_val = stats.ttest_ind(progs_data, stab_data, equal_var=False)  # Assuming variance might not be equal\n",
    "        \n",
    "        # Store results\n",
    "        cols.append(col)\n",
    "        t_stats.append(t_stat)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Combine results into a DataFrame for easy viewing and further analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        'Brain Region': cols,\n",
    "        'T-Statistic': t_stats,\n",
    "        'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Adding the Significant column based on the P-Value\n",
    "results_df['Significant'] = results_df['P-Value'] < 0.05\n",
    "\n",
    "# Display or save the results\n",
    "print(results_df)\n",
    "# Optionally save to a CSV file\n",
    "# results_df.to_csv('t_test_results.csv', index=False)\n",
    "\n",
    "#the resulting negative t stats mean the progs have lower volume than the stab which is good \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prog gpes mean 0.7961809523809524\n",
      "prog gpes std 0.04627393621291616\n",
      "prog cc mean 0.5882318178118178\n",
      "prog cc std 0.0913304093809654\n",
      "stab gpes mean 0.8462126984126985\n",
      "stab gpes std 0.040075420330781784\n",
      "stab cc mean 0.6903541443741444\n",
      "stab cc std 0.07916141540641318\n"
     ]
    }
   ],
   "source": [
    "#calc the means for prog \n",
    "#dont need this because not comparing means \n",
    "#somehow the datatype is list not string \n",
    "gpes_mean = metrics_df['gpe'].mean()\n",
    "gpes_std = metrics_df['gpe'].std()\n",
    "print(f\"prog gpes mean {gpes_mean}\")\n",
    "print(f\"prog gpes std {gpes_std}\")\n",
    "clustering_coefficients_mean = metrics_df['clus_coeff'].mean()\n",
    "clustering_coefficients_std = metrics_df['clus_coeff'].std()\n",
    "print(f\"prog cc mean {clustering_coefficients_mean}\")\n",
    "print(f\"prog cc std {clustering_coefficients_std}\")\n",
    "\n",
    "#stab means \n",
    "stab_gpes_mean = np.mean(stab_gpes)\n",
    "stab_gpes_std = np.std(stab_gpes)\n",
    "print(f\"stab gpes mean {stab_gpes_mean}\")\n",
    "print(f\"stab gpes std {stab_gpes_std}\")\n",
    "stab_clustering_coefficients_mean = np.mean(stab_clustering_coefficients)\n",
    "stab_clustering_coefficients_std = np.std(stab_clustering_coefficients)\n",
    "print(f\"stab cc mean {stab_clustering_coefficients_mean}\")\n",
    "print(f\"stab cc std {stab_clustering_coefficients_std}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ttest_ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mlauber/mci_mri_graph/construct_ggm/create_pop_ggms_bootstrap_streamlined.ipynb Cell 29\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Becho.bumc.bu.edu/home/mlauber/mci_mri_graph/construct_ggm/create_pop_ggms_bootstrap_streamlined.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#now that I have this, just scale it to the whole brains and all patients, and more metrics \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Becho.bumc.bu.edu/home/mlauber/mci_mri_graph/construct_ggm/create_pop_ggms_bootstrap_streamlined.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m teststat,pvalue \u001b[39m=\u001b[39m ttest_ind(gpes_mean,stab_gpes_mean)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Becho.bumc.bu.edu/home/mlauber/mci_mri_graph/construct_ggm/create_pop_ggms_bootstrap_streamlined.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(teststat)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Becho.bumc.bu.edu/home/mlauber/mci_mri_graph/construct_ggm/create_pop_ggms_bootstrap_streamlined.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(pvalue)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ttest_ind' is not defined"
     ]
    }
   ],
   "source": [
    "#now that I have this, just scale it to the whole brains and all patients, and more metrics \n",
    "\n",
    "teststat,pvalue = ttest_ind(gpes_mean,stab_gpes_mean)\n",
    "print(teststat)\n",
    "print(pvalue)\n",
    "\n",
    "teststat,pvalue = ttest_ind(clustering_coefficients_mean,stab_clustering_coefficients_mean)\n",
    "print(teststat)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from this code onwards it's a duplicate of the old stuff "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f92c1c541c642a613962b5b6ff0dc3b5c22792af994e63054619307fd9da051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
