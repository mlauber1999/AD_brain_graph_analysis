{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis   \n",
    "In this notebook we:\n",
    "1. Load the processed tau PET data for ADNI and A4\n",
    "2. Construct the estimated partial correlation graphs\n",
    "3. Conduct statistical analysis of graph level metrics across amyloid groups\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "Analysis steps:\n",
    "\n",
    "1. Load a table with the regional tau SUVr for a given cohort\n",
    "2. Estimate the precision matrix using `GraphicalLasso` from scikit-learn\n",
    "3. Compute the partial correlation matrix from the precision matrix\n",
    "4. Create a Networkx `Graph` from the partial correlation matrix and compute some graph metrics\n",
    "5. Bootstrap: resample the table, and repeat steps 1-4 to estimate the confidence interval on the graph metrics.\n",
    "\n",
    "The functions in this section roughly correspond to the steps just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def small_world_coeff(G, niter=1, nrand=10):\n",
    "    \"\"\"Compute the small world coefficient of a weighted graph G. Average over `nrand` samples of the randomized graph.\"\"\"\n",
    "    Crand = 0\n",
    "    Lrand = 0\n",
    "\n",
    "    for _ in range(nrand):\n",
    "        G_rand = nx.random_reference(G, niter)\n",
    "\n",
    "        Crand += nx.average_clustering(G_rand, weight=\"correlation\")\n",
    "        Lrand += nx.average_shortest_path_length(G_rand, weight=\"distance\")\n",
    "\n",
    "    C = nx.average_clustering(G, weight=\"correlation\")\n",
    "    L = nx.average_shortest_path_length(G, weight=\"distance\")\n",
    "\n",
    "    return (C / Crand) / (L / Lrand)\n",
    "\n",
    "\n",
    "def unweighted_small_world_coeff(G, niter=1, nrand=10):\n",
    "    \"\"\"Compute the small world coefficient of a weighted graph. Average over `nrand` samples of the randomized graph.\"\"\"\n",
    "    Crand = 0\n",
    "    Lrand = 0\n",
    "\n",
    "    for _ in range(nrand):\n",
    "        G_rand = nx.random_reference(G, niter)\n",
    "\n",
    "        Crand += nx.average_clustering(G_rand)\n",
    "        Lrand += nx.average_shortest_path_length(G_rand)\n",
    "\n",
    "    C = nx.average_clustering(G)\n",
    "    L = nx.average_shortest_path_length(G)\n",
    "\n",
    "    return (C / Crand) / (L / Lrand)\n",
    "\n",
    "\n",
    "def partial_correlation(precision):\n",
    "    \"\"\"Compute the partial correlation from a given precision matrix\"\"\"\n",
    "\n",
    "    diag = precision.values.diagonal()\n",
    "\n",
    "    # The - sign is correct, but the diagonal should have 1 instead of -1,\n",
    "    # so we fill it explicitly with ones. The formula on wikipedia only applies\n",
    "    # to off-diagonal elements\n",
    "    partial_correl = -precision.copy() / np.sqrt(diag[:, None] * diag[None, :])\n",
    "\n",
    "    np.fill_diagonal(partial_correl.values, 1)\n",
    "\n",
    "    return partial_correl\n",
    "\n",
    "\n",
    "def pcorr_to_distance(pcorr):\n",
    "    \"\"\"Compute the distance matrix associated to a given partial correlation\n",
    "    matrix: disconnected nodes should stay disconnected, nodes with high\n",
    "    correlation should be close to each other. We also drop connections associated with negative weights\n",
    "    \"\"\"\n",
    "\n",
    "    # return (1 / np.abs(pcorr) - 1).replace({np.inf: 0, -np.inf: 0})\n",
    "\n",
    "    # return (pcorr != 0) * (1 - np.abs(pcorr)) # same as Dyrba 2020, but removing connections set to 0 by lasso\n",
    "\n",
    "    # As explained in the main text, we use arctanh to convert partial correlations into distances\n",
    "    if pcorr <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.arctanh(np.abs(pcorr) - 1)\n",
    "\n",
    "    # return 1 - np.abs(pcorr) # this is what Dyrba 2020 uses, it makes fully connected graphs\n",
    "\n",
    "    # return -np.log(np.abs(pcorr)).replace({np.inf: 0, -np.inf: 0, np.nan:0})\n",
    "\n",
    "\n",
    "def compute_precision(data, params, return_covariance=False):\n",
    "    \"\"\"Takes a dataframe and computes the precision matrix via sklearn.covariance.GraphicalLasso. The data is first transformed via PowerTransformer to ensure normality.\n",
    "\n",
    "    Arguments:\n",
    "    data: dataframe\n",
    "    params: dictionary of paramters passed to GraphicaLasso.\n",
    "    return_covariance: if True, also return the covariance matrix. By default only return precision\n",
    "    \"\"\"\n",
    "\n",
    "    # glasso = make_pipeline(PowerTransformer(), GraphicalLasso(**params))\n",
    "    glasso = make_pipeline(StandardScaler(), GraphicalLasso(**params))\n",
    "\n",
    "    glasso.fit(data)\n",
    "\n",
    "    if return_covariance:\n",
    "        labels = glasso.feature_names_in_\n",
    "        return (\n",
    "            pd.DataFrame(\n",
    "                glasso.named_steps[\"graphicallasso\"].precision_,\n",
    "                index=labels,\n",
    "                columns=labels,\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                glasso.named_steps[\"graphicallasso\"].covariance_,\n",
    "                index=labels,\n",
    "                columns=labels,\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        # GraphicalLasso does not support working natively with dataframes\n",
    "        # we have to restore the column names by hand\n",
    "        labels = glasso.feature_names_in_\n",
    "        return pd.DataFrame(\n",
    "            glasso.named_steps[\"graphicallasso\"].precision_,\n",
    "            index=labels,\n",
    "            columns=labels,\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_partial_correlation(data, params):\n",
    "    \"\"\"Convenience function to compute the partial correlation matrix from data\"\"\"\n",
    "\n",
    "    return partial_correlation(compute_precision(data, params))\n",
    "\n",
    "\n",
    "def precision_to_graph(precision):\n",
    "    \"\"\"Convert the provided precision matrix into a networkx graph.\n",
    "\n",
    "    Arguments:\n",
    "    precision: dataframe, column names will become node labels\n",
    "    allow_self_connections: bool, if False zero out the diagonal\"\"\"\n",
    "\n",
    "    # adjacency matrix\n",
    "    adj = partial_correlation(precision)\n",
    "\n",
    "    graph = nx.from_pandas_adjacency(adj)\n",
    "\n",
    "    # we only included age to control for it, we remove it from the graph before computing graph metrics\n",
    "    graph.remove_node(\"PTAGE\")\n",
    "\n",
    "    # remove self loops\n",
    "    graph.remove_edges_from([(u, v) for u, v in graph.edges() if u == v])\n",
    "\n",
    "    # rename weight to correlation\n",
    "    nx.set_edge_attributes(\n",
    "        graph,\n",
    "        {\n",
    "            (u, v): {\"correlation\": d[\"weight\"]}\n",
    "            for u, v, d in graph.edges(data=True)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # convenience: absolute value of correlation\n",
    "    nx.set_edge_attributes(\n",
    "        graph,\n",
    "        {\n",
    "            (u, v): {\"abs(correlation)\": np.abs(d[\"weight\"])}\n",
    "            for u, v, d in graph.edges(data=True)\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # compute distances from partial correlations\n",
    "    nx.set_edge_attributes(\n",
    "        graph,\n",
    "        name=\"distance\",\n",
    "        values={\n",
    "            (u, v): pcorr_to_distance(weight)\n",
    "            for u, v, weight in graph.edges(data=\"weight\")\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def compute_metrics(graph, metrics):\n",
    "    \"\"\"Compute graph metrics for a networks graph.\n",
    "\n",
    "    Arguments:\n",
    "    graph: networkx graph\n",
    "    metrics: dictionary of metric names and callables\n",
    "\n",
    "    Returns:\n",
    "    res: dictionary of metric names and metric values\"\"\"\n",
    "\n",
    "    return {metric: metrics[metric](graph) for metric in metrics}\n",
    "\n",
    "\n",
    "def data_to_metrics(data, params, metrics, randomize_graph=False):\n",
    "    # convenience function, wrapping all steps into one\n",
    "    precision = compute_precision(data, params=params)\n",
    "\n",
    "    if randomize_graph:\n",
    "        graph = nx.random_reference(precision_to_graph(precision))\n",
    "    else:\n",
    "        graph = precision_to_graph(precision)\n",
    "\n",
    "    return compute_metrics(graph, metrics)\n",
    "\n",
    "\n",
    "def boostrap_graph_metrics(\n",
    "    data, params, metrics, n_samples=1000, randomize_graph=False\n",
    "):\n",
    "    \"\"\"Resample the datframe data, generate graph and compute metrics. Returns a dataframe with the bootstrapped metrics\"\"\"\n",
    "\n",
    "    bootstrap_samples = [\n",
    "        data.sample(frac=1, replace=True) for _ in range(n_samples)\n",
    "    ]\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        res = pool.map(\n",
    "            partial(\n",
    "                data_to_metrics,\n",
    "                params=params,\n",
    "                metrics=metrics,\n",
    "                randomize_graph=randomize_graph,\n",
    "            ),\n",
    "            bootstrap_samples,\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "\n",
    "def bootstrap(data, func, n_samples=1000):\n",
    "    # resample from data and apply fun to each sample\n",
    "    # use this version for general functions rather than graph metrics\n",
    "\n",
    "    bootstrap_samples = [\n",
    "        data.sample(frac=1, replace=True) for _ in range(n_samples)\n",
    "    ]\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        res = pool.map(func, bootstrap_samples)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "base_path = \"/projectnb/vkolagrp/projects/tau_amy_correlation_graphs/jad2024_withdata_followup\"\n",
    "\n",
    "adni = pd.read_csv(\n",
    "        base_path + \"/data_paths_and_cleaning/data/intermediate_data/adni/merged_adni_at_amy_pos_bi_harm.csv\",\n",
    "        dtype={\"RID\": str},\n",
    "    )\n",
    "\n",
    "# normalize the adni values by cerebellum cortex values for better comparison to A4\n",
    "adni = pd.concat(\n",
    "        (\n",
    "            adni[[\"RID\", \"CENTILOIDS\"]],\n",
    "            adni.drop(columns=[\"RID\", \"CENTILOIDS\"]).div(\n",
    "                adni[\"CEREBELLUM_CORTEX\"], axis=0\n",
    "            ),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ).drop(columns=\"CEREBELLUM_CORTEX\")\n",
    "\n",
    "a4 = pd.read_csv(\n",
    "        base_path + \"/data_paths_and_cleaning/data/intermediate_data/a4/merged_a4_at_amy_pos_bi_harm.csv\",\n",
    "        dtype={\"RID\": str},\n",
    "    ).drop(columns=\"CEREBELLUM_CORTEX\")\n",
    "\n",
    "# the demographics data is in a separate file from the regional tau, we will merge them later\n",
    "\n",
    "demo_a4 = pd.read_csv(\n",
    "        base_path + \"/data_paths_and_cleaning/data/demographic_csvs/A4/a4_filtered_demo.csv\",\n",
    "        dtype={\"RID\": str},\n",
    "    )\n",
    "\n",
    "demo_adni = pd.read_csv(\n",
    "        base_path + \"/data_paths_and_cleaning/data/demographic_csvs/ADNI/adni_filtered_demo.csv\",\n",
    "        dtype={\"RID\": str},\n",
    "    )\n",
    "\n",
    "demog = pd.concat([demo_adni, demo_a4], keys=[\"ADNI\", \"A4\"]).reset_index(\n",
    "        level=0, names=\"Dataset\"\n",
    "    )\n",
    "\n",
    "\n",
    "adni_with_demo = pd.merge(adni, demo_adni[[\"RID\", \"PTAGE\"]], on=\"RID\")\n",
    "a4_with_demo = pd.merge(a4, demo_a4[[\"RID\", \"PTAGE\"]], on=\"RID\")\n",
    "\n",
    "# for the next examples, we use the \"high amyloid\" group from ADNI\n",
    "data = adni_with_demo[adni_with_demo[\"CENTILOIDS\"] > 54].drop(\n",
    "        columns=[\"RID\", \"CENTILOIDS\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: estimate the partial correlation graph for one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# parameters for the graphical lasso\n",
    "params = {\n",
    "        \"alpha\": 0.2,\n",
    "        \"max_iter\": 1000,\n",
    "        \"tol\": 1e-3,\n",
    "        \"mode\": \"cd\",\n",
    "        \"eps\": 1e-12,\n",
    "        \"enet_tol\": 1e-7,\n",
    "    }\n",
    "\n",
    "precision, covariance = compute_precision(\n",
    "        data, params, return_covariance=True\n",
    "    )\n",
    "\n",
    "pcorr = partial_correlation(precision)\n",
    "\n",
    "graph = precision_to_graph(precision)\n",
    "\n",
    "pcorr_tall = (\n",
    "        pcorr[(pcorr < 1) & (pcorr != 0)]\n",
    "        .unstack()\n",
    "        .dropna()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Partial Correlation\"})\n",
    "    )\n",
    "\n",
    "mask = np.triu(np.ones_like(pcorr, dtype=bool))\n",
    "pcorr_masked = pcorr.copy()\n",
    "pcorr_masked[mask] = np.nan\n",
    "\n",
    "fig = px.imshow(\n",
    "        pcorr_masked.round(2),\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        color_continuous_scale=\"PiYG\",\n",
    "        color_continuous_midpoint=0,\n",
    "        title=\"Partial Correlation\",\n",
    "        text_auto=True,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "        {\n",
    "            \"plot_bgcolor\": \"white\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Visualize one graph\n",
    "For graphical clarity we use the partial correlation squared as edge thickness, but all metrics are computed using the partial correlation directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "edge_weights = [\n",
    "        1.5 * (2.22 * graph[u][v][\"abs(correlation)\"]) ** 2\n",
    "        for u, v in graph.edges()\n",
    "    ]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "nx.set_edge_attributes(\n",
    "        graph,\n",
    "        {\n",
    "            (u, v): {\"plot_weight\": 0.75 * (2.22 * d[\"abs(correlation)\"]) ** 2}\n",
    "            for u, v, d in graph.edges(data=True)\n",
    "        },\n",
    "    )\n",
    "\n",
    "pos = nx.spectral_layout(graph, weight=\"distance\")\n",
    "pos = nx.spring_layout(graph, pos=pos, weight=\"plot_weight\", iterations=100)\n",
    "\n",
    "nx.draw_networkx_edges(graph, pos, width=edge_weights, ax=ax)\n",
    "\n",
    "nx.draw_networkx_labels(\n",
    "        graph,\n",
    "        pos,\n",
    "        ax=ax,\n",
    "        font_size=8,\n",
    "        bbox=dict(facecolor=\"white\", alpha=1, edgecolor=\"white\", pad=0),\n",
    "    )\n",
    "\n",
    "    # draw edge labels\n",
    "    # edge_labels = nx.get_edge_attributes(graph, 'correlation')\n",
    "    # nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial correlation sparsity with $L_1$ regularization\n",
    "Here we recompute the precision matrix as a function of regularization strength $\\alpha$. We end up selecting $\\alpha = 0.15$ as the optimal value (by BIC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "alphas = np.geomspace(0.15, 1, 16)\n",
    "\n",
    "nz = []\n",
    "\n",
    "df = data\n",
    "\n",
    "for alpha in tqdm(alphas):\n",
    "    params = {\n",
    "        \"alpha\": alpha,\n",
    "        \"max_iter\": 1000,\n",
    "        \"tol\": 1e-3,\n",
    "        \"mode\": \"cd\",\n",
    "        \"eps\": 1e-12,\n",
    "        \"enet_tol\": 1e-7,\n",
    "    }\n",
    "\n",
    "    fun = partial(compute_precision, params=params)\n",
    "\n",
    "    # this is a list, one per bootstrap samples\n",
    "    nonzero_counts = np.array(\n",
    "        list(\n",
    "            map(np.count_nonzero, bootstrap(df, fun, n_samples=1000))\n",
    "        )  # about 60s with n = 128\n",
    "    )\n",
    "\n",
    "    nz.append(\n",
    "        {\n",
    "            \"alpha\": alpha,\n",
    "            \"Median\": np.median(nonzero_counts) / len(df.columns) ** 2,\n",
    "            \"CI_low\": np.quantile(nonzero_counts, 0.25)\n",
    "            / len(df.columns) ** 2,\n",
    "            \"CI_high\": np.quantile(nonzero_counts, 0.75)\n",
    "            / len(df.columns) ** 2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "partial_corr_nz = pd.DataFrame(nz)  # fraction of nonzero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "partial_corr_nz.plot(x=\"alpha\", y=\"Median\")\n",
    "\n",
    "plt.fill_between(\n",
    "    partial_corr_nz.alpha,\n",
    "    partial_corr_nz[\"CI_low\"],\n",
    "    partial_corr_nz[\"CI_high\"],\n",
    "    alpha=0.3,\n",
    "    label='IQR',\n",
    ")\n",
    "\n",
    "plt.axhline(1/len(df.columns),label='Diag. only', color='k',linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(r\"$L_1$ regularization $\\alpha$\")\n",
    "plt.ylabel(\"Partial correlation matrix nonzero fraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amyloid quantiles\n",
    "We split both datasets in tertiles using the ADNI distribution as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_quantiles = 3\n",
    "\n",
    "adni_quantile_labels, adni_amy_bins = pd.qcut(\n",
    "    adni_with_demo[\"CENTILOIDS\"], q=n_quantiles, retbins=True, labels=False\n",
    ")\n",
    "\n",
    "adni_amy_bins\n",
    "\n",
    "a4[\n",
    "    \"CENTILOIDS\"\n",
    "].max()  # this should be less than the adni maximum, which it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a4_quantile_labels = pd.cut(a4[\"CENTILOIDS\"], adni_amy_bins, labels=False)\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    (a4_quantile_labels, adni_quantile_labels), keys=(\"A4\", \"ADNI\")\n",
    ").reset_index(level=0).rename(\n",
    "    columns={\"level_0\": \"Dataset\", \"CENTILOIDS\": \"Centiloid quantile\"}\n",
    ").value_counts().sort_index().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis of graph metrics\n",
    "For each cohort and amyloid tertile, we construct the partial correlation graph and estimate the corresponding graph metrics. The confidence intervals are estimated via boostrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Weighted Clustering Coefficient\": partial(\n",
    "        nx.average_clustering, weight=\"correlation\"\n",
    "    ),\n",
    "    \"Weighted Avg. Shortest Path Length\": partial(\n",
    "        nx.average_shortest_path_length, weight=\"distance\"\n",
    "    ),\n",
    "    \"Weighted Small World\": partial(small_world_coeff, niter=1, nrand=10),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"alpha\": 0.2,\n",
    "    \"max_iter\": 1000,\n",
    "    \"tol\": 1e-3,\n",
    "    \"mode\": \"cd\",\n",
    "    \"eps\": 1e-12,\n",
    "    \"enet_tol\": 1e-7,\n",
    "}\n",
    "\n",
    "# in the paper we use 1000 bootstrap samples, this is just to showcase the method\n",
    "n_boot = 100\n",
    "\n",
    "adni_boot_metrics_results = []\n",
    "a4_boot_metrics_results = []\n",
    "\n",
    "for quantile in tqdm(range(n_quantiles)):\n",
    "    adni_boot_metrics_results.append(\n",
    "        boostrap_graph_metrics(\n",
    "            adni_with_demo[adni_quantile_labels == quantile]\n",
    "            .drop(columns=[\"RID\", \"CENTILOIDS\"])\n",
    "            .dropna(),\n",
    "            params,\n",
    "            metrics,\n",
    "            n_samples=n_boot,\n",
    "            randomize_graph=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "for quantile in tqdm(range(n_quantiles)):\n",
    "    a4_boot_metrics_results.append(\n",
    "        boostrap_graph_metrics(\n",
    "            a4_with_demo[a4_quantile_labels == quantile]\n",
    "            .drop(columns=[\"RID\", \"CENTILOIDS\"])\n",
    "            .dropna(),\n",
    "            params,\n",
    "            metrics,\n",
    "            n_samples=n_boot,\n",
    "            randomize_graph=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "graph_metrics_by_quantile = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.concat(adni_boot_metrics_results, keys=range(n_quantiles))\n",
    "            .reset_index(level=0)\n",
    "            .rename(columns={\"level_0\": \"Centiloid Quantile\"}),\n",
    "            pd.concat(a4_boot_metrics_results, keys=range(n_quantiles))\n",
    "            .reset_index(level=0)\n",
    "            .rename(columns={\"level_0\": \"Centiloid Quantile\"}),\n",
    "        ],\n",
    "        keys=[\"ADNI\", \"A4\"],\n",
    "    )\n",
    "    .reset_index(level=0)\n",
    "    .rename(columns={\"level_0\": \"Dataset\"})\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results and compare cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4), sharex=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.boxplot(\n",
    "        graph_metrics_by_quantile,\n",
    "        x=\"Centiloid Quantile\",\n",
    "        y=metric,\n",
    "        hue=\"Dataset\",\n",
    "        ax=ax.flat[i],\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "ax[0].legend().set_visible(False)\n",
    "ax[1].legend().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
